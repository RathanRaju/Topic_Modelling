{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# from jupytertehmes import jtplot\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, NMF, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot rc parameters\n",
    "\n",
    "# jtplot.style(grid=False)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#464646'\n",
    "#plt.rcParams['axes.edgecolor'] = '#FFFFFF'\n",
    "plt.rcParams['figure.figsize'] = 10, 7\n",
    "plt.rcParams['text.color'] = '#666666'\n",
    "plt.rcParams['axes.labelcolor'] = '#666666'\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.color'] = '#666666'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.color'] = '#666666'\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "# plt.rcParams['font.size'] = 16\n",
    "\n",
    "sns.color_palette('dark')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load news data set\n",
    "# remove meta data headers footers and quotes from news dataset\n",
    "dataset = fetch_20newsgroups(shuffle=True,random_state=32,remove=('headers', 'footers', 'qutes'))\n",
    "\n",
    "#df = fetch_20newsgroups(shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The real question here in my opinion is what Motorola processors running system\n",
      "7 on a MAC are comparable to what Intel processors running Windows on a PC?  I\n",
      "recall there being a conversation here that a 486/25 running Windows benchmarks\n",
      "at about the same speed as 25Mhz 030 in system 7.  I don't know if that is\n",
      "true, but I would love to hear if anyone has any technical data on this.\n",
      "\n",
      "-David \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "Please could someone in the US give me the current street \n",
      "prices on the following, with and without any relevant taxes:\n",
      "\n",
      " 8 Mb 72 pin SIMM\n",
      "16 Mb 72 pin SIMM (both for Mac LC III)\n",
      "\n",
      "Are any tax refunds possible if they are to be exported\n",
      "to the UK? Can you recommend a reliable supplier? \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "Can somebody please help me with information about an\n",
      "American Magnetics Corporation Magstripe Card Reader that\n",
      "I recently bought locally from a surplus dealer.\n",
      "\n",
      "On the rear it has the following information:\n",
      "\n",
      "\tAmerican Magnetics Corporation\n",
      "\tCarson, CA, USA\n",
      "\tMagstripe Card Reader\n",
      "\tModel 41,\n",
      "\tP/N 507500 - 2300112311\n",
      "\n",
      "It is fitted with a cable with a RS232 Cannon 25-pin connector on\n",
      "the end and has a separate power connector like the once used with\n",
      "wall chargers.\n",
      "\n",
      "Frode \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "In article <2077@rwing.UUCP> pat@rwing.UUCP (Pat Myrto) writes:\n",
      ">I am sick, dismayed, discouraged.  And ASHAMED of our Administration.\n",
      ">\n",
      ">Anybody for impeachment?\n",
      "\n",
      "I have already called senators, legislators and the Governor demanding\n",
      "that the warrants be unsealed, and that all involved in this atrocity\n",
      "(including the President, Attorney General and Governor) be suspended\n",
      "pending an investigation.\n",
      "\n",
      "I seriously doubt, however, that anything will ever be done.\n",
      "\n",
      "\n",
      "Welcome to Amerika!\n",
      "\n",
      " \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "From article <1pq6i2$a1f@news.ysu.edu>, by ak296@yfn.ysu.edu (John R. Daker):\n",
      "> \n",
      "> Cup holders (driving is an importantant enough undertaking)\n",
      "> Ashtrays (smokers seem to think it's just fine to use the road)\n",
      "\n",
      "\tOh, sure -- sorry, but the absence of a cupholder is not gonna\n",
      "discourage anyone from eating/drinking in the car;  let's just put one\n",
      "in anyway, so at least they don't have the further distraction of trying\n",
      "not to spill it.\n",
      "\tFurthermore, you are obviously not a smoker; on a cold day, it\n",
      "takes a certain skill to toss a butt out of a cracked window without having\n",
      "it wind-deflect into the back seat.  Also, just 'cause some smokers use\n",
      "the window, doesn't mean all of us do.\n",
      "\tThis reminds me of *one* pleasant feature in the otherwise\n",
      "ergonomically-hellish interior of the Alfa Romeo Milano:  you could ash\n",
      "your cigarette without even removing your hand from the wheel; the 'tray\n",
      "was *right*there*.\n",
      "\n",
      "> Fake convertible roofs and vinyl roofs.\n",
      "> Any gold trim.\n",
      "\n",
      "\tThese, I will agree, are abominations, right along with the fake\n",
      "continental spare-tire kit -- it's sad watching those little old ladies \n",
      "try to load their groceries into the trunk with that huge tire-medallion\n",
      "in the way.\n",
      "\tMost pitiful fake convertible top: on a \"Cadillac\" Cimarron, with\n",
      "all the chrome door trim still visible -- not fooling *anyone*.\n",
      "Of course, there was that Hyundai Excel I once saw... \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "In article <C5r5vt.941@news.cso.uiuc.edu> cka52397@uxa.cso.uiuc.edu (OrioleFan@uiuc) writes:\n",
      ">jmann@vineland.pubs.stratus.com (Jim Mann) writes:\n",
      "\n",
      "[deleted]\n",
      "\n",
      ">\tSomeone told me this game started at 10:05 cdt.  Is this true??/ Who\n",
      ">in their right mind would go to a game on monday at 11AM????\n",
      "\n",
      "Keep in mind this was in Massachussetts.  Today was Patriots Day, a state\n",
      "holiday.  I think it might be a floating holiday, but given that the\n",
      "Marathon also happens the same day, most people don't go in.\n",
      "\n",
      "\n",
      "-- \n",
      "\n",
      "#include <std_disclaimer.h> \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "I have this used equipment for sale, everything is negotiable!\n",
      "\n",
      "1200 Baud Compuadd Internal Modem, all docs and software    $ 25.00\n",
      "\n",
      "SCO UNIX V3.2.2 All disks and Docs (Has UUCP/all Utils)     $150.00\n",
      "\n",
      "Old 1.2MB floppy drive, functional, out of an old 286.      $ 20.00\n",
      "\n",
      "Dead ST1196 80MB RLL drive, don't know whats wrong with it. $ 20.00\n",
      "\n",
      "Old Joystick, don't remember the brand name                 $ 10.00\n",
      "\n",
      "Old Boat Anchor CGA Monitor with full length CGA CArd       $ 20.00\n",
      "\n",
      "Serial Card 25 Pin                                          $ 10.00\n",
      "\n",
      "Test Drive III Accolade                                     $ 20.00 \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "I just recently bought a 4 MB ram card for my original mac portable \n",
      "(backlit) and have since had some bizarre crashes. It happens when I put \n",
      "the machine to sleep and wake the machine up. sometimes it will just \n",
      "freeze the cursor and lock the machine up forcing me to push the reset \n",
      "switch. Other times it will give me the usual bomb box with the error \n",
      "message of \"Co processor not installed\". \n",
      "\n",
      "I know one solution is NOT to put the machine to sleep, but does anyone \n",
      "have any ideas on what could be causing this or better yet what might fix \n",
      "it? The memory card is Psuedostatic ram and goes into the PDS Slot. That \n",
      "probably figures into the problem. the manufacturer is King Memory (Not \n",
      "kingston) from irvine, CA. They say the problem is in my machine. \n",
      "\n",
      "Any Ideas? -- Gene Wright.\n",
      " \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "In article <Apr.5.23.31.36.1993.23919@athos.rutgers.edu> by028@cleveland.freenet.edu (Gary V. Cavano) writes:\n",
      ">I'm new to this group, and maybe this has been covered already,\n",
      ">but does anybody out there see the current emphasis on the\n",
      ">environment being turned (unintentionally, of course) into\n",
      ">pantheism?\n",
      "\n",
      "Yes.\n",
      "\n",
      "(I am adamantly an environmentalist.  I will not use styrofoam table service.\n",
      "Please keep that in mind as you read this post - I do not wish to attack\n",
      "environmentalism)\n",
      "\n",
      "A half truth is at least as dangerous as a complete lie.  A complete lie will\n",
      "rarely be readily accepted, while a half truth (the lie subtly hidden) is more\n",
      "powerfully offered by one who masquerades as an angel of light.\n",
      "\n",
      "Satan has (for some people) loosened the grip on treating the earth as something\n",
      "other than God's intricate handiwork, something other than that on which the\n",
      "health of future generations is based.  It is being treated with respect.  You\n",
      "think he's going to happily leave it at that?  No.  When one error is rejected,\n",
      "it is his style to push people to the opposite error.  Therefore the earth is\n",
      "not God's intricate handiwork, not because it is rubbish, but because it is\n",
      "God.  Mother earth is the one you are to primarily love and serve.\n",
      "\n",
      "I see two facets of a response to it:\n",
      "\n",
      "1: Care for the environment.  Treat it with proper respect, both because it is\n",
      "   God's intricate handiwork and the health of future generation, and because\n",
      "   showing the facet of one who is disregardful of such things does not\n",
      "   constitute what the Apostle Paul called \"becoming all things to all men so\n",
      "   that by all possible means I might save some.\"\n",
      "\n",
      "   Don't say \"Forget the environment, I've got important things to spend my time\n",
      "   on.\" - putting your foot in your mouth in this manner will destroy your\n",
      "   credibility in expressing the things that _are_ more important.\n",
      "\n",
      "2: Show that it is not the ultimate entity, that it is creature and not\n",
      "   creator.  Show that its beauty and glory points to a greater beauty and\n",
      "   glory.  Show that it is not the ultimate tapestry, but one of many cords\n",
      "   woven in the infinite tapestry. \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n",
      "In <Apr.22.00.57.03.1993.2118@geneva.rutgers.edu> jprzybyl@skidmore.edu writes:\n",
      "\n",
      "> I may be wrong, but wasn't Jeff Fenholt part of Black Sabbath?  He's a\n",
      "> MAJOR brother in Christ now.  He totally changed his life around, and\n",
      "\n",
      "      Why should he have been any different \"then\"? Ozzy Osbourne,\n",
      "ex-singer and main character of the Black Sabbath of good ole days past,\n",
      "is and always was a devout catholic. Or so I've heard over on the\n",
      "alt.rock-n-roll.metal newsgroups, an' I figure those folks oughta know..\n",
      " \n",
      "\n",
      " #################################################################################################### \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sneak peek of the news articles\n",
    "for idx in range(10):\n",
    "    print(dataset.data[idx],'\\n\\n','#'*100, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put your data into a dataframe\n",
    "news_df = pd.DataFrame({'News': dataset.data,\n",
    "                       'Target': dataset.target})\n",
    "\n",
    "# get dimensions of data \n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The real question here in my opinion is what M...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please could someone in the US give me the cur...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can somebody please help me with information a...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In article &lt;2077@rwing.UUCP&gt; pat@rwing.UUCP (P...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;1pq6i2$a1f@news.ysu.edu&gt;, by ak2...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target\n",
       "0  The real question here in my opinion is what M...       4\n",
       "1  Please could someone in the US give me the cur...       4\n",
       "2  Can somebody please help me with information a...      12\n",
       "3  In article <2077@rwing.UUCP> pat@rwing.UUCP (P...      16\n",
       "4  From article <1pq6i2$a1f@news.ysu.edu>, by ak2...       7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace target names from target numbers in our news data frame\n",
    "news_df['Target_name'] = news_df['Target'].apply(lambda x: dataset.target_names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The real question here in my opinion is what M...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please could someone in the US give me the cur...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can somebody please help me with information a...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In article &lt;2077@rwing.UUCP&gt; pat@rwing.UUCP (P...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;1pq6i2$a1f@news.ysu.edu&gt;, by ak2...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target  \\\n",
       "0  The real question here in my opinion is what M...       4   \n",
       "1  Please could someone in the US give me the cur...       4   \n",
       "2  Can somebody please help me with information a...      12   \n",
       "3  In article <2077@rwing.UUCP> pat@rwing.UUCP (P...      16   \n",
       "4  From article <1pq6i2$a1f@news.ysu.edu>, by ak2...       7   \n",
       "\n",
       "             Target_name  \n",
       "0  comp.sys.mac.hardware  \n",
       "1  comp.sys.mac.hardware  \n",
       "2        sci.electronics  \n",
       "3     talk.politics.guns  \n",
       "4              rec.autos  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:59<00:00, 189.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# clean text data\n",
    "# remove non alphabetic characters\n",
    "# remove stopwords and lemmatize\n",
    "\n",
    "def clean_text(sentence):\n",
    "    # remove non alphabetic sequences\n",
    "    pattern = re.compile(r'[^a-z]+')\n",
    "    sentence = sentence.lower()\n",
    "    sentence = pattern.sub(' ', sentence).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    word_list = word_tokenize(sentence)\n",
    "    \n",
    "    # stop words\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    # puctuation\n",
    "    # punct = set(string.punctuation)\n",
    "    \n",
    "    # remove stop words\n",
    "    word_list = [word for word in word_list if word not in stopwords_list]\n",
    "    # remove very small words, length < 3\n",
    "    # they don't contribute any useful information\n",
    "    word_list = [word for word in word_list if len(word) > 2]\n",
    "    # remove punctuation\n",
    "    # word_list = [word for word in word_list if word not in punct]\n",
    "    \n",
    "    # stemming\n",
    "    # ps  = PorterStemmer()\n",
    "    # word_list = [ps.stem(word) for word in word_list]\n",
    "    \n",
    "    # lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    word_list = [lemma.lemmatize(word) for word in word_list]\n",
    "    # list to sentence\n",
    "    sentence = ' '.join(word_list)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# we'll use tqdm to monitor progress of data cleaning process\n",
    "# create tqdm for pandas\n",
    "tqdm.pandas()\n",
    "# clean text data\n",
    "news_df['News'] = news_df['News'].progress_apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>real question opinion motorola processor runni...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please could someone give current street price...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>somebody please help information american magn...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article rwing uucp pat rwing uucp pat myrto wr...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article news ysu edu yfn ysu edu john daker cu...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target  \\\n",
       "0  real question opinion motorola processor runni...       4   \n",
       "1  please could someone give current street price...       4   \n",
       "2  somebody please help information american magn...      12   \n",
       "3  article rwing uucp pat rwing uucp pat myrto wr...      16   \n",
       "4  article news ysu edu yfn ysu edu john daker cu...       7   \n",
       "\n",
       "             Target_name  \n",
       "0  comp.sys.mac.hardware  \n",
       "1  comp.sys.mac.hardware  \n",
       "2        sci.electronics  \n",
       "3     talk.politics.guns  \n",
       "4              rec.autos  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news_df['News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = news_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1697,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1698,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919, 63936)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start getting some features\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919, 63936)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we use tf-idf features\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919, 63936)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00778518, 0.00778518, 0.00778518, ..., 0.00778518, 0.00778518,\n",
       "        0.00778518],\n",
       "       [0.00942305, 0.00942305, 0.00942305, ..., 0.00942305, 0.00942305,\n",
       "        0.00942305],\n",
       "       [0.01003448, 0.01003448, 0.01003448, ..., 0.01003448, 0.01003448,\n",
       "        0.01003448],\n",
       "       ...,\n",
       "       [0.00695686, 0.00695686, 0.00695686, ..., 0.00695686, 0.00695686,\n",
       "        0.00695686],\n",
       "       [0.00638792, 0.00638792, 0.00638792, ..., 0.00638792, 0.00638792,\n",
       "        0.00638792],\n",
       "       [0.01066029, 0.01066029, 0.01066029, ..., 0.01066029, 0.01066029,\n",
       "        0.01066029]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda instance\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                     random_state=12,\n",
    "                                     learning_method='online',\n",
    "                                     max_iter=5,\n",
    "                                \n",
    "                                     learning_offset=50)\n",
    "# fit model\n",
    "lda_model.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 63936)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lda_x is your doc-topic distribution that you can use for feature vector to your SVM model.\n",
    "# lda.components_ is your topic-word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919, 20)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document topic matrix\n",
    "lda_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919, 20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word topic matrix\n",
    "doc_topic_lda = lda_model.transform(X_train_tfidf)\n",
    "doc_topic_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ndarray.view>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(lda_x, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'peculiar color problem macx apple macintosh server like know others seen problem happened current version version type client window displayed part window wrong color window moved slightly forcing server repaint repainted correct color happen xterm window happened graphic window motif client'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-0347dca2647d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=np.float64, order=\"C\",\n\u001b[1;32m--> 447\u001b[1;33m                         accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    766\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    767\u001b[0m         \"\"\"\n\u001b[1;32m--> 768\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'peculiar color problem macx apple macintosh server like know others seen problem happened current version version type client window displayed part window wrong color window moved slightly forcing server repaint repainted correct color happen xterm window happened graphic window motif client'"
     ]
    }
   ],
   "source": [
    "predictions = svm_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize text data\n",
    "tfid_vec = TfidfVectorizer(tokenizer=lambda x: str(x).split())\n",
    "X = tfid_vec.fit_transform(news_df['News'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "tsne = TSNE(n_components=2,\n",
    "           perplexity=50,\n",
    "           learning_rate=300,\n",
    "           n_iter=800,\n",
    "           verbose=1)\n",
    "# tsne to our document vectors\n",
    "componets = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = news_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create svd instance\n",
    "svd_model = TruncatedSVD(n_components=20,\n",
    "                         random_state=12,\n",
    "                         n_iter=100,\n",
    "                         algorithm='randomized')\n",
    "\n",
    "# fit model to data\n",
    "svd_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic word mapping martrix\n",
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document topic mapping matrix\n",
    "doc_topic = svd_model.fit_transform(X)\n",
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfid_vec.get_feature_names()\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map words to topics\n",
    "def map_word2topic(components, terms):\n",
    "    # create output series\n",
    "    word2topics = pd.Series()\n",
    "    \n",
    "    for idx, component in enumerate(components):\n",
    "        # map terms (words) with topic\n",
    "        # which is probability of word given a topic P(w|t)\n",
    "        term_topic = pd.Series(component, index=terms)\n",
    "        # sort values based on probability\n",
    "        term_topic.sort_values(ascending=False, inplace=True)\n",
    "        # put result in series output\n",
    "        word2topics['topic '+str(idx)] = list(term_topic.iloc[:10].index)\n",
    "        \n",
    "    return word2topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2topics = map_word2topic(svd_model.components_, terms)\n",
    "\n",
    "# print topic results\n",
    "print('Topics\\t\\tWords')\n",
    "for idx, item in zip(word2topics.index, word2topics):\n",
    "    print(idx,'\\t',item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top3 topics for a news document\n",
    "def get_top3_topics(x):\n",
    "    top3 = list(x.sort_values(ascending=False).head(3).index) + list(x.sort_values(ascending=False).head(3).values)\n",
    "    return top3\n",
    "\n",
    "# map top3 topic words to news document\n",
    "def map_topicword2doc(model, X):\n",
    "    # output data frame column list\n",
    "    cols = ['topic_'+str(i+1)+'_name' for i in range(3)] + ['topic_'+str(i+1)+'_prob' for i in range(3)]\n",
    "    # doc to topic mapping\n",
    "    doc_topic = model.fit_transform(X)\n",
    "    # list of topics\n",
    "    topics = ['topic'+str(i) for i in range(20)]\n",
    "    # doc topic data frame\n",
    "    doc_topic_df = pd.DataFrame(doc_topic, columns=topics)\n",
    "    # map top 3 topics to doc\n",
    "    outdf = doc_topic_df.progress_apply(lambda x: get_top3_topics(x), axis=1)\n",
    "    # outdf is a series of list\n",
    "    # convert it to a data frame\n",
    "    outdf = pd.DataFrame(dict(zip(outdf.index, outdf.values))).T\n",
    "    outdf.columns = cols\n",
    "    \n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = map_topicword2doc(svd_model, X)\n",
    "news_topics = pd.concat([news_df, top_topics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics.shape, news_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert probability from string to float\n",
    "news_topics = news_topics.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplot of top 3 topic scores to check their distribution\n",
    "cols = ['topic_1_prob','topic_2_prob','topic_3_prob']\n",
    "colors = [sns.xkcd_rgb['greenish cyan'], sns.xkcd_rgb['cyan'], sns.xkcd_rgb['reddish pink']]\n",
    "fig = plt.figure(figsize=[15,8])\n",
    "news_topics.boxplot(column=cols,\n",
    "                   grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda instance\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                     random_state=12,\n",
    "                                     learning_method='online',\n",
    "                                     max_iter=5,\n",
    "                                     learning_offset=50)\n",
    "# fit model\n",
    "lda_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                \n",
    "                                stop_words='english')\n",
    "#t0 = time()\n",
    "#tf = tf_vectorizer.fit_transform(data_samples)\n",
    "X = tf_vectorizer.fit_transform(news_df['News'])\n",
    "#print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# vectorize text data\n",
    "#tfid_vec = TfidfVectorizer(tokenizer=lambda x: str(x).split())\n",
    "#X = tfid_vec.fit_transform(news_df['News'])\n",
    "X.shape\n",
    "\n",
    "print(\"Topic modelling with LDA...\")\n",
    "lda_model = LatentDirichletAllocation(n_components=20,\n",
    "                                     random_state=12,\n",
    "                                     learning_method='online',\n",
    "                                     max_iter=5,\n",
    "                                     learning_offset=50)\n",
    "\n",
    "\n",
    "lda_x = lda_model.fit_transform(X)\n",
    "# so lda_x is your doc-topic distribution that you can use for feature vector to your SVM model.\n",
    "# lda.components_ is your topic-word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lda_x is your doc-topic distribution that you can use for feature vector to your SVM model.\n",
    "# lda.components_ is your topic-word distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic word mapping martrix\n",
    "lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document topic mapping matrix\n",
    "doc_topic_lda = lda_model.transform(X)\n",
    "doc_topic_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map topics to terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to map words to topics\n",
    "def map_word2topic(components, terms):\n",
    "    # create output series\n",
    "    word2topics = pd.Series()\n",
    "    \n",
    "    for idx, component in enumerate(components):\n",
    "        # map terms (words) with topic\n",
    "        # which is probability of word given a topic P(w|t)\n",
    "        term_topic = pd.Series(component, index=terms)\n",
    "        # sort values based on probability\n",
    "        term_topic.sort_values(ascending=False, inplace=True)\n",
    "        # put result in series output\n",
    "        word2topics['topic '+str(idx)] = list(term_topic.iloc[:10].index)\n",
    "        \n",
    "    return word2topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfid_vec.get_feature_names()\n",
    "len(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2topics_lda = map_word2topic(lda_model.components_, terms)\n",
    "\n",
    "# print topic results\n",
    "print('Topics\\t\\tWords')\n",
    "for idx, item in zip(word2topics_lda.index, word2topics_lda):\n",
    "    print(idx,'\\t',item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic word mapping martrix\n",
    "lda_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document topic mapping matrix\n",
    "doc_topic_lda = lda_model.transform(X)\n",
    "doc_topic_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2topics_lda = map_word2topic(lda_model.components_, terms)\n",
    "\n",
    "# print topic results\n",
    "print('Topics\\t\\tWords')\n",
    "for idx, item in zip(word2topics_lda.index, word2topics_lda):\n",
    "    print(idx,'\\t',item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# from jupytertehmes import jtplot\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, NMF, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plot rc parameters\n",
    "\n",
    "# jtplot.style(grid=False)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#464646'\n",
    "#plt.rcParams['axes.edgecolor'] = '#FFFFFF'\n",
    "plt.rcParams['figure.figsize'] = 10, 7\n",
    "plt.rcParams['text.color'] = '#666666'\n",
    "plt.rcParams['axes.labelcolor'] = '#666666'\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.color'] = '#666666'\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.color'] = '#666666'\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "# plt.rcParams['font.size'] = 16\n",
    "\n",
    "sns.color_palette('dark')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load news data set\n",
    "# remove meta data headers footers and quotes from news dataset\n",
    "df = fetch_20newsgroups(shuffle=True,random_state=32,remove=('headers', 'footers', 'qutes'))\n",
    "\n",
    "#df = fetch_20newsgroups(shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your data into a dataframe\n",
    "df2 = pd.DataFrame({'News': df.data,\n",
    "                       'Target': df.target})\n",
    "\n",
    "# get dimensions of data \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "df2['News'] = \\\n",
    "df2['News'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "df2['News'] = \\\n",
    "df2['News'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "df2['News'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(df2['News'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data = df2.News.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       alpha='symmetric',\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
